---
title: "Visualizing Popper Pipelines"
teaching: 0
exercises: 0
questions:
- "How can I quickly get a sense of what a pipeline does?"
objectives:
- "Implement and visualize an end-to-end simple experimentation 
  pipeline."
keypoints:
- "The `popper` command allows easy visualization of what a pipeline 
  does."
- "Top-level structure of a pipeline can be arbitrarily extended by 
  scripts of any language."
---

At the end of the previous section we learned how to create a 
"popperized" repo with one empty pipeline in it. We will now populate 
some of these scripts that get generated by the `popper init` command 
with shell commands that implement an actual pipeline. We will 
implement a pipeline that obtains a 
[dataset](https://github.com/datasets/co2-fossil-global) of global CO2 
emissions from fossil fuels since 1751, and obtain the mean of per 
capita emissions.

```bash
popper init co2-emissions
$ tree .
.
└── pipelines
    └── co2-emissions
        ├── README.md
        ├── post-run.sh
        ├── run.sh
        ├── setup.sh
        ├── teardown.sh
        └── validate.sh
```

The pipeline consists of 3 stages:

  * Setup. Fetch the CSV file and pre-process it so it is ready for 
    our purposes.

  * Run. Run a simple python program that groups per capita emissions 
    every 5 years and calculates the mean.

  * Validate. We check that the output is as expected and generate a 
    table that can be used in an article.

As you can see, not all the scripts generated by the `popper init` 
command have corresponding steps from above. We will remove those:

```bash
rm pipelines/co2-emissions/post-run.sh pipelines/co2-emissions/teardown.sh
```

> **NOTE**: version 0.6 of Popper will provide the ability to give 
arbitrary names and execution order for the stages involved in a 
pipeline.

### Setup

In order to download a public dataset, we can make use of `wget` or 
`curl`:

```bash
curl -LO https://github.com/datasets/co2-fossil-global/raw/master/global.csv
```

This dataset has a `Per Capita` column that only contains values from 
1950. In order to make it easier to be processed, we'll add zeros to 
all the missing values.

```python
#!/usr/bin/env python
import csv
import sys

fname = sys.argv[1]
fout = fname.replace('.csv', '') + '_clean.csv'

with open(fname, 'r') as fi, open(fout, 'w') as fo:
    r = csv.reader(fi)
    w = csv.writer(fo)

    # get 0-based index of last column in CSV file
    last = len(next(r)) - 1

    # go back to first line
    fi.seek(0)

    for row in r:
        if not row[last]:
            row[last] = 0
        w.writerow(row)
```

We store the above in a `pipelines/co2-emissions/scripts/add_zeros.py` 
python file and make it executable (0755 permissions). Putting these 
together, the `setup.sh` stage looks like the following:

```bash
#!/bin/bash
# [wf] obtain and clean dataset
set -ex

# [wf] create data folder if it doesn't exist
mkdir -p data/

# [wf] download dataset from github
curl -L \
  -o data/global.csv \
  https://github.com/datasets/co2-fossil-global/raw/master/global.csv

# [wf] add zeros to missing per capita column values
scripts/add_zeros.py data/global.csv
```

### Run

We would like to group the data every 5 years and obtain the main. We 
can do this with the following Python script:

```python
#!/usr/bin/env python
import csv
import sys

from itertools import izip_longest

fname = sys.argv[1]
group_size = int(sys.argv[2])
fout = fname.replace('_clean.csv', '') + '_per_capita_mean.csv'


def grouper(iterable, n, fillvalue=None):
    args = [iter(iterable)] * n
    return izip_longest(*args, fillvalue=fillvalue)


with open(fname, 'r') as fi, open(fout, 'w') as fo:
    r = csv.reader(fi)
    w = csv.writer(fo)

    # get 0-based index of last column in CSV file
    last = len(next(r)) - 1

    for g in grouper(r, group_size):
        group_sum = 0
        year = 0

        for row in g:
            group_sum += float(row[last])
            year = row[0]

        w.writerow([year, group_sum / 5.0])
```

we store the above script in a 
`pipelines/co2-emissions/scripts/get_mean.py` (and make it 
executable). And the `run.sh` script is simple and looks like the 
following:

```bash
#!/bin/bash
# [wf] obtain n-year means
set -ex

# [wf] group every n years and obtain mean over each group
scripts/get_mean.py data/global_clean.csv 5
```

### Validate

In this phase, we generate a table in Markdown format from the CSV 
output we obtained in the previous step. We'll create an executable 
`pipelines/co2-emissions/scripts/get_mdown_table.py` script containing 
the following:

```python
#!/usr/bin/env python
import csv
import sys

fname = sys.argv[1]


with open(fname, 'r') as fi:
    r = csv.reader(fi)

    print('| Year | Mean |')
    print('| ---- | ---- |')

    for row in r:
        print('| {} |'.format(' | '.join(row)))
```

We would also like to ensure that we generated data as we expected it 
on the previous step:

```python
#!/usr/bin/env python
import csv
import sys

fname = sys.argv[1]


with open(fname, 'r') as fi:
    r = csv.reader(fi)

    # get 0-based index of last column in CSV file
    last = len(next(r)) - 1

    for row in r:
        # for years greater than 1950, we should have non-zero mean
        if int(row[0]) < 1950:
            assert float(row[last]) == 0.0
        else:
            assert float(row[last]) != 0
```

We store the above script in a 
`pipelines/co2-emissions/scripts/validate_output.py` python script and 
make it executable. Putting all together we have the `validate.sh` 
stage:

```bash
#!/bin/bash
# [wf] validate results and get a table
set -ex

# [wf] verify that we got actual result values
scripts/validate_output.py data/global_per_capita_mean.csv

# [wf] generate markdown table
scripts/get_mdown_table.py data/global_per_capita_mean.csv
```

### Test and Commit

After we write all the commands above, we can test them by running 
`popper check`:

```bash
cd pipelines/co2-emissions
popper check
```

once all runs OK, we can then commit to the repository:

```bash
cd ../../
git add .
git commit -m "adding co2-emissions pipeline"
```

# Visualize a Pipeline

As mentioned in the previous chapter, a useful way of abstracting 
scientific explorations is by thinking in terms of a generic pipeline. 
While bash scripts from popper pipelines are simple to read, it is 
useful to have a way of quickly visualizing what a pipeline does. 
Popper provides the option of generating a call graph for pipelines.

As you may have noticed, some of the comments of the bash scripts we 
created for all the three stages start with the `[wf]` prefix. The 
`popper` command can generate a diagram of the call graph for the 
pipeline in order to visualize what it does. The `[wf]` comments 
generate nodes in the call graph, following the convention specified 
[here](https://github.com/systemslab/popper/issues/190#issue-276733567).

To generate a graph for this pipeline, execute the following:

```bash
popper workflow co2-emissions
```

The above generates a graph in `.dot` format. To visualize it, you can 
install the [`graphviz`](https://graphviz.gitlab.io/) package and 
execute:

```bash
popper workflow co2-emissions | dot -T png -o wf.png | open wf.png
```

Alternatively you can use the <http://www.webgraphviz.com/> website to 
generate a graph by copy-pasting the output of the `popper workflow` 
command.

> ## Get The Pipeline
>
> The pipeline we created in this example is available in [this 
> repo](https://github.com/popperized/lesson-episode-3).
{: .callout}
